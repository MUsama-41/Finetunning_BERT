{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformer"
      ],
      "metadata": {
        "id": "y2226fq9kI3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1o7_-E3j8k5",
        "outputId": "c439e5ea-9e19-4159-9f83-850b1ab60a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: The quick brown fox jumps over the lazy dog.\n",
            "Embedding length: 768\n",
            "\n",
            "Sentence: Hello world! This is a test sentence.\n",
            "Embedding length: 768\n",
            "\n",
            "Sentence: Transformers are a type of neural network architecture.\n",
            "Embedding length: 768\n",
            "\n",
            "Sentence: Natural Language Processing is a fascinating field.\n",
            "Embedding length: 768\n",
            "\n",
            "Sentence: OpenAI develops artificial intelligence technologies.\n",
            "Embedding length: 768\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Enable hidden states output\n",
        "model.config.output_hidden_states = True\n",
        "\n",
        "# Prepare input sentences\n",
        "def tokenize_sentences(sentences, tokenizer, max_length=128):\n",
        "    return tokenizer(sentences, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Hello world! This is a test sentence.\",\n",
        "    \"Transformers are a type of neural network architecture.\",\n",
        "    \"Natural Language Processing is a fascinating field.\",\n",
        "    \"OpenAI develops artificial intelligence technologies.\"\n",
        "]\n",
        "\n",
        "inputs = tokenize_sentences(sentences, tokenizer)\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Extract hidden states\n",
        "hidden_states = outputs.hidden_states\n",
        "last_hidden_state = hidden_states[-1]\n",
        "\n",
        "# Define mean pooling function\n",
        "def mean_pooling(token_embeddings, attention_mask):\n",
        "    attention_mask = attention_mask.unsqueeze(-1)\n",
        "    summed = torch.sum(token_embeddings * attention_mask, 1)\n",
        "    count = torch.sum(attention_mask, 1)\n",
        "    return summed / count\n",
        "\n",
        "# Compute sentence embeddings\n",
        "sentence_embeddings = mean_pooling(last_hidden_state, inputs['attention_mask'])\n",
        "sentence_embeddings_np = sentence_embeddings.detach().numpy()\n",
        "\n",
        "# Print embeddings\n",
        "def print_embeddings(sentences, embeddings):\n",
        "    for sentence, embedding in zip(sentences, embeddings):\n",
        "        print(f\"Sentence: {sentence}\")\n",
        "        print(f\"Embedding length: {len(embedding)}\\n\")\n",
        "\n",
        "print_embeddings(sentences, sentence_embeddings_np)\n"
      ]
    }
  ]
}